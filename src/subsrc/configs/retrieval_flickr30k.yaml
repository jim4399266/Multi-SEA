## General Setup
datasets: ['flickr30k']
data_root: "/home/tzj/datas/flickr30k_karpathy_prepared"
output_dir: "../outputs_flickr30k"
log_dir: "../logs_flickr30k"


test_checkpoints_dir: "/home/tzj/codes/my_retrieval_model/checkpoint/flickr30k"
checkpoint: ''
arch: 'aformer3_swiglu_queue_new'
desc: ''
seed: 42
train_dataset_len: -1
val_dataset_len: -1
test_dataset_len: -1
shuffle: True
task_name: {'irtr': 1}
get_recall_metric: True
top_k: 64
queue_size: 1000
distill: False
momentum: 0.995
alpha: 0.4
negative_all_rank: True
coco_scale: [''] # Use 1k, 5k, or both for the test set on MSCOCO

# ----------------------  Lightning Trainer Setting  ------------------------------
num_sanity_val_steps: 0
fast_dev_run: False
# val_check_interval: 1.0
check_val_every_n_epoch: 10
accelerator: 'gpu'
devices: [0]
batch_size: 500  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.
per_gpu_batch_size: 20  # you should define this manually with per_gpu_batch_size=#
num_nodes: 1
pin_memory: True
num_workers: 8
precision: 32
max_grad_norm: 1.
max_epoch: 200
max_steps: -1
warmup_steps: 0.15

# ----------------------  Image Setting  ----------------------
image_encoder_config:
  vit_name: 'ViT-B-16'
  vit: '/home/tzj/pretrained_models/ViT/ViT-B-16.pt'
  image_size: 224
  patch_size: 16

  train_transform_keys: ["clip"]
  val_transform_keys: ["clip"]


# ----------------------  Text Setting  ----------------------
text_encoder_config:

  tokenizer_name: 'roberta-base'
  tokenizer: '/home/tzj/pretrained_models/en-roberta-base'


  max_text_len: 40
  whole_word_masking: False # note that whole_word_masking does not work for RoBERTa
  mlm_prob: 0.15

# ----------------------  Transformer Setting  ----------------------
aformer_config_path: 'subsrc/configs/aformer_config.json'
num_top_layer: 4  # A-Former layers
hidden_size: 768
attention_groups: False
beta: 1.0
num_heads: 12

mlp_ratio: 4 # dim of middle layer ï¼šhidden_size * mlp_ratio
drop_rate: 0.1 # dropout

# ----------------------  Optimizer Setting  ----------------------
optimizer:
  optim_type: 'adamw'
  init_lr: 1e-5
  #learning_rate: 1e-5,
  min_lr: 0
  eps: 1e-8
  betas: (0.9, 0.98)
  weight_decay: 0.05

  lr_mult_head: 5 # multiply lr for downstream heads
  lr_mult_cross_modal: 5  # multiply lr for the cross-modal module

#scheduler: 'linear'

scheduler: 'cosine'
num_cycles: 0.3
#scheduler: {
#  sched: cosine,
#  lr: 1e-5,
#  min_lr: 1e-6,
#  decay_rate: 1,
#  warmup_lr: 1e-5,
#  warmup_epochs: 1,
#  cooldown_epochs: 0,
#}
